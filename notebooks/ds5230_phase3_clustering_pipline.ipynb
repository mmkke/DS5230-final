{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# umap\n",
    "import umap\n",
    "from sklearn.manifold import trustworthiness\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# clustering\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Start Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track runtime\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "nb_path = Path(os.getcwd())\n",
    "print(nb_path)\n",
    "path = str(nb_path.parent)\n",
    "print(path)\n",
    "\n",
    "# path to figs folder\n",
    "figs_path = path + '/figs'\n",
    "\n",
    "# path to data\n",
    "data_path= path + '/data'\n",
    "\n",
    "# path to src folder\n",
    "src_path = path + '/src'\n",
    "print(src_path)\n",
    "\n",
    "# sys path\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Import Util Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_dim_red(cap_x):\n",
    "    ''' \n",
    "    Description:\n",
    "    Params:\n",
    "    Returns:\n",
    "    '''\n",
    "    \n",
    "    # create umap object\n",
    "    reducer = umap.UMAP()\n",
    "\n",
    "    # fit and embed\n",
    "    reducer.fit(cap_x)\n",
    "    embedding = reducer.transform(cap_x)\n",
    "\n",
    "    # verify results\n",
    "    assert(np.all(embedding == reducer.embedding_))\n",
    "\n",
    "    # get params\n",
    "    params = reducer.get_params()\n",
    "\n",
    "    \n",
    "    # trustworthiness\n",
    "    cap_x_dist = squareform(pdist(cap_x))\n",
    "    cap_x_dist_embed = squareform(pdist(embedding))\n",
    "    trust = trustworthiness(X=cap_x_dist, \n",
    "                             X_embedded=cap_x_dist_embed, \n",
    "                             n_neighbors=params['n_neighbors'],\n",
    "                             metric=params['metric'])\n",
    "\n",
    "    results_dict = {\n",
    "    'embedding' : embedding,\n",
    "    'n_neighbors' : params['n_neighbors'],\n",
    "    'min_dist' : params['min_dist'],\n",
    "    'metric' : params['metric'],\n",
    "    'n_components': params['n_components'],\n",
    "    'trustworthiness' : trust\n",
    "    }\n",
    "\n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_kmeans(cap_x, n_clusters, df_row_dict_list):\n",
    "    '''\n",
    "    Description: Performs k-means clustering.\n",
    "\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "    Input:\n",
    "            cap_x: embedding (ndarray)\n",
    "            n_cclusters: value for n_clusters (int)\n",
    "            df_row_dict_list = list for dicts of kmeans results    \n",
    "    Returns:\n",
    "            df_row_dict_list.append({\n",
    "                                    'n_clusters': n_clusters,\n",
    "                                    'inertia': inertia,\n",
    "                                    'calinski_harabasz_score': indices_dict['calinski_harabasz_score'],\n",
    "                                    'davies_bouldin_score': indices_dict['davies_bouldin_score'],\n",
    "                                    'silhouette_score': indices_dict['silhouette_score']\n",
    "                                }\n",
    "    '''\n",
    "    # define kmeans object and set params\n",
    "    kmeans = KMeans()\n",
    "\n",
    "    # fit k means\n",
    "    kmeans.fit_predict(cap_x)\n",
    "\n",
    "\n",
    "    # get lables and inertia\n",
    "    labels = kmeans.labels_\n",
    "    inertia = kmeans.inertia_\n",
    "\n",
    "    # internal indices\n",
    "    indices_dict = kmeans_indices(cap_x, labels)\n",
    "\n",
    "    # add values to dict list\n",
    "    df_row_dict_list.append(\n",
    "        {\n",
    "            'n_clusters': n_clusters,\n",
    "            'inertia': inertia,\n",
    "            'calinski_harabasz_score': indices_dict['calinski_harabasz_score'],\n",
    "            'davies_bouldin_score': indices_dict['davies_bouldin_score'],\n",
    "            'silhouette_score': indices_dict['silhouette_score'],\n",
    "            'cluster_labels': labels\n",
    "        }\n",
    "    )\n",
    "    return df_row_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(results_dict):\n",
    "    ''' \n",
    "    Description:\n",
    "    Params:\n",
    "    Returns:\n",
    "    '''\n",
    "    print('*'*100)\n",
    "    print('*'*100)\n",
    "    print('Hyperparameters:')\n",
    "    print('n_neighbors: ', results_dict['n_neighbors'])\n",
    "    print('min_dist: ', results_dict['min_dist'])\n",
    "    print('metric: ', results_dict['metric'])\n",
    "    print('n_components: ', results_dict['n_components'])\n",
    "\n",
    "\n",
    "    # set umap embedding as cap_x\n",
    "    cap_x = results_dict['embedding']\n",
    "\n",
    "    # choose a range of n_clusters to try for kmeans\n",
    "    n_clusters_list = np.arange(2, 16, 1)\n",
    "\n",
    "    # init dict\n",
    "    df_row_dict_list = []\n",
    "\n",
    "    # iterate over values of n in n_clusters_list\n",
    "    for n_clusters in n_clusters_list:\n",
    "\n",
    "        # run clusters for values of n\n",
    "        # results for each iteration collected in df_row_dict_list \n",
    "        cluster_kmeans(cap_x, n_clusters, df_row_dict_list)\n",
    "    \n",
    "    # convert results dicts to dataframe\n",
    "    results_df = pd.DataFrame(df_row_dict_list)\n",
    "\n",
    "    # determine elbow location\n",
    "    n_clusters_found = find_elbow(results_df, sensitivity=1.0)\n",
    "\n",
    "    ## hopkins statistic\n",
    "    cap_h = get_hopkins(cap_x)\n",
    "    print(f\"Hopkin's Statistic = {cap_h}\")\n",
    "    \n",
    "    ## testing KMEANS using internal indicies\n",
    "    n_clusters_db_score_is_min = results_df.loc[results_df['davies_bouldin_score'].idxmin(), 'n_clusters']\n",
    "    n_clusters_ch_score_is_max = results_df.loc[results_df['calinski_harabasz_score'].idxmax(), 'n_clusters']\n",
    "    n_clusters_silhouette_score_is_max = results_df.loc[results_df['silhouette_score'].idxmax(), 'n_clusters']\n",
    "    sil_score = results_df.loc[results_df['silhouette_score'].idxmax(), 'silhouette_score']\n",
    "    cluster_labels = results_df.loc[results_df['n_clusters'] == n_clusters_found, 'cluster_labels']\n",
    "\n",
    "    # will return valid results in df_row_dict\n",
    "    df_row_dict = {\n",
    "        'algo': 'k_means',\n",
    "        'n_clusters_found' : n_clusters_found,\n",
    "        'n_clusters_db_score_is_min' : n_clusters_db_score_is_min,\n",
    "        'n_clusters_ch_score_is_max' : n_clusters_ch_score_is_max,\n",
    "        'n_clusters_silhouette_score_is_max' : n_clusters_silhouette_score_is_max,\n",
    "        'silhouette_score' : sil_score,\n",
    "        'hopkins_statistic' : cap_h,\n",
    "        'umap_n_neighbors' : results_dict['n_neighbors'],\n",
    "        'umap_min_dist' : results_dict['min_dist'],\n",
    "        'umap_metric' : results_dict['metric'],\n",
    "        'umap_n_components' : results_dict['n_components'],\n",
    "        'trustworthiness' : results_dict['trustworthiness'],\n",
    "        'eps' : np.nan,\n",
    "        'dbscan_min_samples' : np.nan,\n",
    "        'validity_index' : np.nan,\n",
    "        'cluster_labels': cluster_labels,\n",
    "        'embedding' : cap_x\n",
    "        }\n",
    "    \n",
    "    # test1\n",
    "    if n_clusters_found == n_clusters_db_score_is_min == n_clusters_ch_score_is_max == n_clusters_silhouette_score_is_max:\n",
    "        print(\"Test1 Pass: Kmeans successfully clustered.\")\n",
    "        print('Number of Clusters: ', n_clusters_found)\n",
    "        return df_row_dict, True\n",
    "    # test2\n",
    "    if  n_clusters_db_score_is_min == n_clusters_ch_score_is_max == n_clusters_silhouette_score_is_max:\n",
    "        print(\"Test2 Pass: Kmeans successfully clustered.\")\n",
    "        print('Number of Clusters: ', n_clusters_found)\n",
    "        return df_row_dict, True\n",
    "    else:\n",
    "        print(\"Fail: Kmeans did not successfully cluster.\")\n",
    "        return df_row_dict, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(results_dict):\n",
    "    ''' \n",
    "    Description:\n",
    "    Params:\n",
    "    Returns:\n",
    "    '''\n",
    "    \n",
    "    # set umap embedding as cap_x\n",
    "    cap_x = results_dict['embedding']\n",
    "\n",
    "    # get eps and min_samples from knee locator\n",
    "    eps, min_samples = find_eps(cap_x)\n",
    "\n",
    "    # iterate over a range near eps to find best eps value, determined by valididty score\n",
    "    eps_scan_range = [0.8, 1.8, 0.1]\n",
    "    f_eps_list = factor_eps(eps, eps_scan_range)\n",
    "    \n",
    "    # iterate dbscan over the eps values in f_eps_list\n",
    "    results_df = cluster_dbscan(cap_x, f_eps_list, min_samples)\n",
    "    \n",
    "    # get values where validy score is greatest\n",
    "    validity_index = results_df.loc[results_df['validity_index'].idxmax(), 'validity_index']\n",
    "    eps = results_df.loc[results_df['validity_index'].idxmax(), 'k_dist_eps']\n",
    "    min_samples = results_df.loc[results_df['validity_index'].idxmax(), 'min_samples']\n",
    "    n_clusters_found = results_df.loc[results_df['validity_index'].idxmax(), 'n_clusters']\n",
    "    cluster_label = results_df.loc[results_df['validity_index'].idxmax(), 'cluster_labels']\n",
    "\n",
    "    print('DBSCAN')\n",
    "    print('Number of Clusters: ', n_clusters_found)\n",
    "    print('Validity Index: ', validity_index)\n",
    "\n",
    "    # return results in df_row_dict\n",
    "    df_row_dict = {\n",
    "            'algo': 'dbscan',\n",
    "            'n_clusters_found' : n_clusters_found,\n",
    "            'n_clusters_db_score_is_min' : np.nan,\n",
    "            'n_clusters_ch_score_is_max' : np.nan,\n",
    "            'n_clusters_silhouette_score_is_max' : np.nan,\n",
    "            'silhouette_score' : np.nan,\n",
    "            'hopkins_statistic' : results_dict['hopkins_statistic'],\n",
    "            'umap_n_neighbors' : results_dict['umap_n_neighbors'],\n",
    "            'umap_min_dist' : results_dict['umap_min_dist'],\n",
    "            'umap_metric' : results_dict['umap_metric'],\n",
    "            'umap_n_components' : results_dict['umap_n_components'],\n",
    "            'trustworthiness' : results_dict['trustworthiness'],\n",
    "            'eps' : eps,\n",
    "            'dbscan_min_samples' : min_samples,\n",
    "            'validity_index' : validity_index,\n",
    "            'cluster_labels': cluster_label\n",
    "            }\n",
    "\n",
    "\n",
    "    return df_row_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed data csv file name\n",
    "data_file = \"/curated/trans_data_design.csv\"\n",
    "\n",
    "# read in design matrix dataframe\n",
    "design_matrix  = pd.read_csv( data_path + data_file )\n",
    "\n",
    "\n",
    "# target vector csv file name\n",
    "target_file = \"/curated/beans_target.csv\"\n",
    "\n",
    "\n",
    "# read in encoded target vector dataframe\n",
    "target_vector = pd.read_csv( data_path + target_file, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Check Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(design_matrix) == len(target_vector)\n",
    "\n",
    "print(\"Design Matrix Shape:\", design_matrix.shape)\n",
    "print(\"Target Vector Shape:\", target_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relable target vector, we can go back an change this later\n",
    "target_vector = target_vector.rename(columns={'id': 'ID', 'target_encoded': 'Target'})\n",
    "target_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Shuffle Data Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ID columns\n",
    "cols = list(design_matrix.columns)\n",
    "design_matrix['ID'] = np.arange(0, len(design_matrix))\n",
    "col_order = ['ID'] + cols\n",
    "design_matrix = design_matrix[col_order]\n",
    "design_matrix_shuffled = shuffle(design_matrix, random_state=42, n_samples=None)\n",
    "design_matrix_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vector_shuffled = shuffle(target_vector, random_state=42, n_samples=None)\n",
    "target_vector_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Convert Feature Matrix to ndarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop ID col\n",
    "design_matrix_shuffled_noID = design_matrix_shuffled.drop('ID', axis=1)\n",
    "\n",
    "# convert to ndarray\n",
    "cap_x = design_matrix_shuffled_noID.to_numpy()\n",
    "print(f'cap_x shape: {cap_x.shape}')\n",
    "cap_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## UMAP (default params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = umap_dim_red(cap_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## K-Means  (default params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict, kmeans_solution = kmeans(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## DBSCAN  (default params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kmeans_solution:\n",
    "    results_df = pd.DataFrame(results_dict)\n",
    "else:\n",
    "    results_dict = dbscan(results_dict)\n",
    "    results_df = pd.DataFrame(results_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish = time.time()\n",
    "hours = int((finish - start) // 3600)\n",
    "minutes = int(((finish - start) % 3600) // 60)\n",
    "seconds = int((finish - start) % 60)\n",
    "print(f\"Total Run Time(hh:mm.ss): {hours:02d}:{minutes:02d}.{seconds:02d}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usml_base_ds2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
